{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CIFAR10 데이터를 활용한 ResNet 구현 및 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qz4Fs_7dM09r",
    "outputId": "c16f688b-b564-415f-f376-cb9a3f66c693"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "#google drive mount\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "kOfnZGc4Xgim"
   },
   "outputs": [],
   "source": [
    "#libraray\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "13jAFD65KitE"
   },
   "outputs": [],
   "source": [
    "#가중치 초기화\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.kaiming_normal_(m.weight, mode= 'fan_out', nonlinearity='relu')\n",
    "        if m.bias is not None:\n",
    "                nn.init.constant_(m.bias,0)\n",
    "    elif isinstance(m, nn.BatchNorm2d):\n",
    "        nn.init.constant_(m.weight, 1)\n",
    "        nn.init.constant_(m.bias, 0)\n",
    "    elif isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, 0, 0.01)\n",
    "        nn.init.constant_(m.bias,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xAUo4UaqkPUM"
   },
   "outputs": [],
   "source": [
    "#data transformer\n",
    "transformer = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4), # 입력 사이즈가 작아 crop 시 padding을 줌.\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 103,
     "referenced_widgets": [
      "090a3eb5c89d428abb284556df2b4e6c",
      "d7b9cb9984924749b2d05a45dabf3529",
      "4a47abf147cd488c872092dd541e6cf3",
      "16e0866dd52c415a88edbe4b7e324253",
      "033ea924f4de4a85902272c3b74cefbe",
      "03804903fb2345c6a2d0bd1926bbb0ca",
      "666fdad5a54044e291ced11586b40e95",
      "356adf57b38e44f2879e3fc13d83949d",
      "474070fdd538445ca578d21d25ea89c8",
      "dbefad12d706444d8c69eb794ef1c800",
      "26d1bc0dedcd43e1bebed25ab67fdef4"
     ]
    },
    "id": "mfpDvKwXrBZ3",
    "outputId": "ade999ba-df57-4d59-f092-df59cb0febfa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "090a3eb5c89d428abb284556df2b4e6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/170498071 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists('/content/data'):\n",
    "    os.mkdir('/content/data')\n",
    "\n",
    "# data load\n",
    "train_data = datasets.CIFAR10(root='./data', train=True, download=True, transform=transformer)\n",
    "test_data = datasets.CIFAR10(root='./data', train=False, download=True, transform=transformer)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_data, batch_size=256, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ksvl2j4TnfcA",
    "outputId": "edbe1357-63ce-4193-b202-0765db21b50d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "#hyperparameter\n",
    "num_epochs = 50\n",
    "batch_size = 256\n",
    "learning_rate = 0.1\n",
    "\n",
    "class_list = train_data.classes\n",
    "num_claases = len(class_list)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(class_list)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Residual Block 구조 정의\n",
    "class BasicBlock(nn.Module): #ResNet18, ResNet34\n",
    "    mul = 1 #기본 구조에서는 out_chs의 수가 바뀌지 않음으로 1로 설정\n",
    "    def __init__(self, in_chs:int, out_chs:int, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        # 입력과 출력의 높이와 너비가 동일하고 identity mapping과의 연산을 위해 채널 수도 동일하게 조정 \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_chs, out_channels=out_chs, kernel_size= 3, stride=stride, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_chs, out_chs, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_chs)\n",
    "        )\n",
    "\n",
    "        #identity map\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        # F(x)와 x의 size가 안 맞을 경우, x의 모양을 맞춰 줌\n",
    "        if stride != 1:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_chs, out_chs, kernel_size=1, stride = stride, bias=False),\n",
    "                nn.BatchNorm2d(out_chs)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class BottleNeck(nn.Module):#ResNet layer 50 이상\n",
    "    mul = 4 \n",
    "    def __init__(self, in_chs:int, out_chs:int, stride=1):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_chs, out_chs, kernel_size=1, stride=stride, bias=False), #BatchNorm에 편향이 있음으로 bias= False\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(out_chs, out_chs, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_chs),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(out_chs, out_chs*BottleNeck.mul, kernel_size=1, stride=1, bias=False),\n",
    "            nn.BatchNorm2d(out_chs*BottleNeck.mul)\n",
    "        )\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "\n",
    "        if stride != 1 or out_chs*BottleNeck.mul != in_chs:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_chs, out_chs*BottleNeck.mul, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_chs*BottleNeck.mul)\n",
    "            )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2(out)\n",
    "        out = self.conv3(out)\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 모델 생성\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block: BasicBlock or BottleNeck , num_blocks:list, num_classes=10):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 첫 conv layer num_out_chs 설정  \n",
    "        self.init_out_chs = 64\n",
    "\n",
    "        # 일반적인 conv layer\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3, self.init_out_chs, kernel_size=7, stride=2, padding=3),\n",
    "            nn.BatchNorm2d(self.init_out_chs),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        )\n",
    "\n",
    "        # residual conv block\n",
    "        self.conv2_x = self.make_layer(block, 64, num_blocks[0], stride=1)\n",
    "        self.conv3_x = self.make_layer(block, 128, num_blocks[1], stride=2)\n",
    "        self.conv4_x = self.make_layer(block, 256, num_blocks[2], stride=2)\n",
    "        self.conv5_x = self.make_layer(block, 512, num_blocks[3], stride=2)\n",
    "\n",
    "        # avg, fc layers\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1,1))\n",
    "        self.linear = nn.Linear(512*block.mul, num_classes)\n",
    "\n",
    "    def make_layer(self, block, out_chs, blocks:int, stride):\n",
    "        # 첫 layer만 크기를 줄이고, 그 다음은 모양을 유지\n",
    "        strides = [stride] + [1]*(blocks-1)\n",
    "        layers = []\n",
    "        for i in range(blocks):\n",
    "            layers.append(block(self.init_out_chs, out_chs, strides[i]))\n",
    "            self.init_out_chs = block.mul * out_chs\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.conv2_x(out)\n",
    "        out = self.conv3_x(out)\n",
    "        out = self.conv4_x(out)\n",
    "        out = self.conv5_x(out)\n",
    "        out = self.avgpool(out)\n",
    "        out = torch.flatten(out, 1) #(m,512) or (m,2048)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18():\n",
    "    return ResNet(BasicBlock, [2, 2, 2, 2])\n",
    "\n",
    "def ResNet34():\n",
    "    return ResNet(BasicBlock, [3, 4, 6, 3])\n",
    "\n",
    "def ResNet50():\n",
    "    return ResNet(BottleNeck, [3, 4, 6, 3])\n",
    "\n",
    "def ResNet101():\n",
    "    return ResNet(BottleNeck, [3, 4, 23, 3])\n",
    "\n",
    "def ResNet152():\n",
    "    return ResNet(BottleNeck, [3, 8, 36, 3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V54n7cnMsVm5",
    "outputId": "3799a0c5-c1c8-4704-d6f0-42fd374a1671"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2_x): Sequential(\n",
       "    (0): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (conv3_x): Sequential(\n",
       "    (0): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (conv4_x): Sequential(\n",
       "    (0): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (3): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (4): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (5): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (conv5_x): Sequential(\n",
       "    (0): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "    (2): BottleNeck(\n",
       "      (conv1): Sequential(\n",
       "        (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv2): Sequential(\n",
       "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (2): ReLU()\n",
       "      )\n",
       "      (conv3): Sequential(\n",
       "        (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (shortcut): Sequential()\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (linear): Linear(in_features=2048, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ResNet50으로 학습 진행\n",
    "model = ResNet50().to(device)\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UWr1dqshscdx",
    "outputId": "466d3f9d-a6eb-4aab-d39c-f0665aa6825a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1          [256, 64, 16, 16]           9,472\n",
      "       BatchNorm2d-2          [256, 64, 16, 16]             128\n",
      "              ReLU-3          [256, 64, 16, 16]               0\n",
      "         MaxPool2d-4            [256, 64, 8, 8]               0\n",
      "            Conv2d-5            [256, 64, 8, 8]           4,096\n",
      "       BatchNorm2d-6            [256, 64, 8, 8]             128\n",
      "              ReLU-7            [256, 64, 8, 8]               0\n",
      "            Conv2d-8            [256, 64, 8, 8]          36,864\n",
      "       BatchNorm2d-9            [256, 64, 8, 8]             128\n",
      "             ReLU-10            [256, 64, 8, 8]               0\n",
      "           Conv2d-11           [256, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-12           [256, 256, 8, 8]             512\n",
      "           Conv2d-13           [256, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-14           [256, 256, 8, 8]             512\n",
      "       BottleNeck-15           [256, 256, 8, 8]               0\n",
      "           Conv2d-16            [256, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-17            [256, 64, 8, 8]             128\n",
      "             ReLU-18            [256, 64, 8, 8]               0\n",
      "           Conv2d-19            [256, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-20            [256, 64, 8, 8]             128\n",
      "             ReLU-21            [256, 64, 8, 8]               0\n",
      "           Conv2d-22           [256, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-23           [256, 256, 8, 8]             512\n",
      "       BottleNeck-24           [256, 256, 8, 8]               0\n",
      "           Conv2d-25            [256, 64, 8, 8]          16,384\n",
      "      BatchNorm2d-26            [256, 64, 8, 8]             128\n",
      "             ReLU-27            [256, 64, 8, 8]               0\n",
      "           Conv2d-28            [256, 64, 8, 8]          36,864\n",
      "      BatchNorm2d-29            [256, 64, 8, 8]             128\n",
      "             ReLU-30            [256, 64, 8, 8]               0\n",
      "           Conv2d-31           [256, 256, 8, 8]          16,384\n",
      "      BatchNorm2d-32           [256, 256, 8, 8]             512\n",
      "       BottleNeck-33           [256, 256, 8, 8]               0\n",
      "           Conv2d-34           [256, 128, 4, 4]          32,768\n",
      "      BatchNorm2d-35           [256, 128, 4, 4]             256\n",
      "             ReLU-36           [256, 128, 4, 4]               0\n",
      "           Conv2d-37           [256, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-38           [256, 128, 4, 4]             256\n",
      "             ReLU-39           [256, 128, 4, 4]               0\n",
      "           Conv2d-40           [256, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-41           [256, 512, 4, 4]           1,024\n",
      "           Conv2d-42           [256, 512, 4, 4]         131,072\n",
      "      BatchNorm2d-43           [256, 512, 4, 4]           1,024\n",
      "       BottleNeck-44           [256, 512, 4, 4]               0\n",
      "           Conv2d-45           [256, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-46           [256, 128, 4, 4]             256\n",
      "             ReLU-47           [256, 128, 4, 4]               0\n",
      "           Conv2d-48           [256, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-49           [256, 128, 4, 4]             256\n",
      "             ReLU-50           [256, 128, 4, 4]               0\n",
      "           Conv2d-51           [256, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-52           [256, 512, 4, 4]           1,024\n",
      "       BottleNeck-53           [256, 512, 4, 4]               0\n",
      "           Conv2d-54           [256, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-55           [256, 128, 4, 4]             256\n",
      "             ReLU-56           [256, 128, 4, 4]               0\n",
      "           Conv2d-57           [256, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-58           [256, 128, 4, 4]             256\n",
      "             ReLU-59           [256, 128, 4, 4]               0\n",
      "           Conv2d-60           [256, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-61           [256, 512, 4, 4]           1,024\n",
      "       BottleNeck-62           [256, 512, 4, 4]               0\n",
      "           Conv2d-63           [256, 128, 4, 4]          65,536\n",
      "      BatchNorm2d-64           [256, 128, 4, 4]             256\n",
      "             ReLU-65           [256, 128, 4, 4]               0\n",
      "           Conv2d-66           [256, 128, 4, 4]         147,456\n",
      "      BatchNorm2d-67           [256, 128, 4, 4]             256\n",
      "             ReLU-68           [256, 128, 4, 4]               0\n",
      "           Conv2d-69           [256, 512, 4, 4]          65,536\n",
      "      BatchNorm2d-70           [256, 512, 4, 4]           1,024\n",
      "       BottleNeck-71           [256, 512, 4, 4]               0\n",
      "           Conv2d-72           [256, 256, 2, 2]         131,072\n",
      "      BatchNorm2d-73           [256, 256, 2, 2]             512\n",
      "             ReLU-74           [256, 256, 2, 2]               0\n",
      "           Conv2d-75           [256, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-76           [256, 256, 2, 2]             512\n",
      "             ReLU-77           [256, 256, 2, 2]               0\n",
      "           Conv2d-78          [256, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-79          [256, 1024, 2, 2]           2,048\n",
      "           Conv2d-80          [256, 1024, 2, 2]         524,288\n",
      "      BatchNorm2d-81          [256, 1024, 2, 2]           2,048\n",
      "       BottleNeck-82          [256, 1024, 2, 2]               0\n",
      "           Conv2d-83           [256, 256, 2, 2]         262,144\n",
      "      BatchNorm2d-84           [256, 256, 2, 2]             512\n",
      "             ReLU-85           [256, 256, 2, 2]               0\n",
      "           Conv2d-86           [256, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-87           [256, 256, 2, 2]             512\n",
      "             ReLU-88           [256, 256, 2, 2]               0\n",
      "           Conv2d-89          [256, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-90          [256, 1024, 2, 2]           2,048\n",
      "       BottleNeck-91          [256, 1024, 2, 2]               0\n",
      "           Conv2d-92           [256, 256, 2, 2]         262,144\n",
      "      BatchNorm2d-93           [256, 256, 2, 2]             512\n",
      "             ReLU-94           [256, 256, 2, 2]               0\n",
      "           Conv2d-95           [256, 256, 2, 2]         589,824\n",
      "      BatchNorm2d-96           [256, 256, 2, 2]             512\n",
      "             ReLU-97           [256, 256, 2, 2]               0\n",
      "           Conv2d-98          [256, 1024, 2, 2]         262,144\n",
      "      BatchNorm2d-99          [256, 1024, 2, 2]           2,048\n",
      "      BottleNeck-100          [256, 1024, 2, 2]               0\n",
      "          Conv2d-101           [256, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-102           [256, 256, 2, 2]             512\n",
      "            ReLU-103           [256, 256, 2, 2]               0\n",
      "          Conv2d-104           [256, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-105           [256, 256, 2, 2]             512\n",
      "            ReLU-106           [256, 256, 2, 2]               0\n",
      "          Conv2d-107          [256, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-108          [256, 1024, 2, 2]           2,048\n",
      "      BottleNeck-109          [256, 1024, 2, 2]               0\n",
      "          Conv2d-110           [256, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-111           [256, 256, 2, 2]             512\n",
      "            ReLU-112           [256, 256, 2, 2]               0\n",
      "          Conv2d-113           [256, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-114           [256, 256, 2, 2]             512\n",
      "            ReLU-115           [256, 256, 2, 2]               0\n",
      "          Conv2d-116          [256, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-117          [256, 1024, 2, 2]           2,048\n",
      "      BottleNeck-118          [256, 1024, 2, 2]               0\n",
      "          Conv2d-119           [256, 256, 2, 2]         262,144\n",
      "     BatchNorm2d-120           [256, 256, 2, 2]             512\n",
      "            ReLU-121           [256, 256, 2, 2]               0\n",
      "          Conv2d-122           [256, 256, 2, 2]         589,824\n",
      "     BatchNorm2d-123           [256, 256, 2, 2]             512\n",
      "            ReLU-124           [256, 256, 2, 2]               0\n",
      "          Conv2d-125          [256, 1024, 2, 2]         262,144\n",
      "     BatchNorm2d-126          [256, 1024, 2, 2]           2,048\n",
      "      BottleNeck-127          [256, 1024, 2, 2]               0\n",
      "          Conv2d-128           [256, 512, 1, 1]         524,288\n",
      "     BatchNorm2d-129           [256, 512, 1, 1]           1,024\n",
      "            ReLU-130           [256, 512, 1, 1]               0\n",
      "          Conv2d-131           [256, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-132           [256, 512, 1, 1]           1,024\n",
      "            ReLU-133           [256, 512, 1, 1]               0\n",
      "          Conv2d-134          [256, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-135          [256, 2048, 1, 1]           4,096\n",
      "          Conv2d-136          [256, 2048, 1, 1]       2,097,152\n",
      "     BatchNorm2d-137          [256, 2048, 1, 1]           4,096\n",
      "      BottleNeck-138          [256, 2048, 1, 1]               0\n",
      "          Conv2d-139           [256, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-140           [256, 512, 1, 1]           1,024\n",
      "            ReLU-141           [256, 512, 1, 1]               0\n",
      "          Conv2d-142           [256, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-143           [256, 512, 1, 1]           1,024\n",
      "            ReLU-144           [256, 512, 1, 1]               0\n",
      "          Conv2d-145          [256, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-146          [256, 2048, 1, 1]           4,096\n",
      "      BottleNeck-147          [256, 2048, 1, 1]               0\n",
      "          Conv2d-148           [256, 512, 1, 1]       1,048,576\n",
      "     BatchNorm2d-149           [256, 512, 1, 1]           1,024\n",
      "            ReLU-150           [256, 512, 1, 1]               0\n",
      "          Conv2d-151           [256, 512, 1, 1]       2,359,296\n",
      "     BatchNorm2d-152           [256, 512, 1, 1]           1,024\n",
      "            ReLU-153           [256, 512, 1, 1]               0\n",
      "          Conv2d-154          [256, 2048, 1, 1]       1,048,576\n",
      "     BatchNorm2d-155          [256, 2048, 1, 1]           4,096\n",
      "      BottleNeck-156          [256, 2048, 1, 1]               0\n",
      "AdaptiveAvgPool2d-157          [256, 2048, 1, 1]               0\n",
      "          Linear-158                  [256, 10]          20,490\n",
      "================================================================\n",
      "Total params: 23,528,586\n",
      "Trainable params: 23,528,586\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.00\n",
      "Forward/backward pass size (MB): 1218.02\n",
      "Params size (MB): 89.75\n",
      "Estimated Total Size (MB): 1310.77\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(model, (3,32,32), batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "b3jGxx14pk7e"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate, \n",
    "                      momentum=0.9, weight_decay=0.0001)\n",
    "\n",
    "#논문에서는 60만번 중 error plateaus가 나오면 0.1 씩 곱해줬으나, 코랩에선 훈련 한계가 있어 \n",
    "#30번마다 0.3 씩 곱해주는 걸로 설정\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "b444CeyAqfxI"
   },
   "outputs": [],
   "source": [
    "#train\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data,target)  in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        logits = model(data)\n",
    "        loss = criterion(logits, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch_idx+1)%30 == 0:\n",
    "            print(f'Train Epoch: {epoch} ({100*(batch_idx+1)*batch_size/len(train_loader.dataset):.0f})%',\n",
    "                  f'Loss: {loss}')\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "dLTKwMzDtshy"
   },
   "outputs": [],
   "source": [
    "#test\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss=0\n",
    "    correct=0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target)\n",
    "            pred = output.max(1)[1]\n",
    "            correct += pred.eq(target).sum().item()\n",
    "        \n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"\\nTest set: Average Loss:{test_loss:.4f}, Accuracy: {100 * correct / len(test_loader.dataset)}%\\n\")\n",
    "        print('='*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5mvSpriD1KlH",
    "outputId": "60f88b00-91d7-45f0-9053-c1ee4b9db973"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 (15)% Loss: 8.546749114990234\n",
      "Train Epoch: 1 (31)% Loss: 4.19499397277832\n",
      "Train Epoch: 1 (46)% Loss: 2.293015956878662\n",
      "Train Epoch: 1 (61)% Loss: 2.397780656814575\n",
      "Train Epoch: 1 (77)% Loss: 2.3078858852386475\n",
      "Train Epoch: 1 (92)% Loss: 2.449352741241455\n",
      "\n",
      "Test set: Average Loss:0.0132, Accuracy: 17.65%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 2 (15)% Loss: 2.467991828918457\n",
      "Train Epoch: 2 (31)% Loss: 2.2306644916534424\n",
      "Train Epoch: 2 (46)% Loss: 2.5731008052825928\n",
      "Train Epoch: 2 (61)% Loss: 2.1204001903533936\n",
      "Train Epoch: 2 (77)% Loss: 2.4383256435394287\n",
      "Train Epoch: 2 (92)% Loss: 2.0913021564483643\n",
      "\n",
      "Test set: Average Loss:0.0091, Accuracy: 20.99%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 3 (15)% Loss: 2.11258864402771\n",
      "Train Epoch: 3 (31)% Loss: 2.135343551635742\n",
      "Train Epoch: 3 (46)% Loss: 2.1017465591430664\n",
      "Train Epoch: 3 (61)% Loss: 2.1499738693237305\n",
      "Train Epoch: 3 (77)% Loss: 2.0656702518463135\n",
      "Train Epoch: 3 (92)% Loss: 2.0645227432250977\n",
      "\n",
      "Test set: Average Loss:0.0092, Accuracy: 24.46%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 4 (15)% Loss: 1.9411613941192627\n",
      "Train Epoch: 4 (31)% Loss: 1.9166067838668823\n",
      "Train Epoch: 4 (46)% Loss: 1.969355583190918\n",
      "Train Epoch: 4 (61)% Loss: 1.8423885107040405\n",
      "Train Epoch: 4 (77)% Loss: 1.854727864265442\n",
      "Train Epoch: 4 (92)% Loss: 1.8612685203552246\n",
      "\n",
      "Test set: Average Loss:0.0083, Accuracy: 28.09%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 5 (15)% Loss: 1.8965959548950195\n",
      "Train Epoch: 5 (31)% Loss: 1.7940412759780884\n",
      "Train Epoch: 5 (46)% Loss: 1.9035147428512573\n",
      "Train Epoch: 5 (61)% Loss: 1.854438304901123\n",
      "Train Epoch: 5 (77)% Loss: 1.7632691860198975\n",
      "Train Epoch: 5 (92)% Loss: 1.8761361837387085\n",
      "\n",
      "Test set: Average Loss:0.0079, Accuracy: 31.17%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 6 (15)% Loss: 1.7445523738861084\n",
      "Train Epoch: 6 (31)% Loss: 1.938331127166748\n",
      "Train Epoch: 6 (46)% Loss: 1.8626326322555542\n",
      "Train Epoch: 6 (61)% Loss: 1.8422785997390747\n",
      "Train Epoch: 6 (77)% Loss: 1.676684856414795\n",
      "Train Epoch: 6 (92)% Loss: 1.7068201303482056\n",
      "\n",
      "Test set: Average Loss:0.0076, Accuracy: 31.72%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 7 (15)% Loss: 1.8947713375091553\n",
      "Train Epoch: 7 (31)% Loss: 1.747615933418274\n",
      "Train Epoch: 7 (46)% Loss: 1.5916767120361328\n",
      "Train Epoch: 7 (61)% Loss: 1.7316688299179077\n",
      "Train Epoch: 7 (77)% Loss: 1.753591775894165\n",
      "Train Epoch: 7 (92)% Loss: 1.6801403760910034\n",
      "\n",
      "Test set: Average Loss:0.0075, Accuracy: 36.87%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 8 (15)% Loss: 1.738034725189209\n",
      "Train Epoch: 8 (31)% Loss: 1.7601380348205566\n",
      "Train Epoch: 8 (46)% Loss: 1.7343814373016357\n",
      "Train Epoch: 8 (61)% Loss: 1.675889015197754\n",
      "Train Epoch: 8 (77)% Loss: 1.7478306293487549\n",
      "Train Epoch: 8 (92)% Loss: 1.6080328226089478\n",
      "\n",
      "Test set: Average Loss:0.0070, Accuracy: 37.51%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 9 (15)% Loss: 1.6993846893310547\n",
      "Train Epoch: 9 (31)% Loss: 1.702602505683899\n",
      "Train Epoch: 9 (46)% Loss: 1.778914451599121\n",
      "Train Epoch: 9 (61)% Loss: 1.597030758857727\n",
      "Train Epoch: 9 (77)% Loss: 1.7774895429611206\n",
      "Train Epoch: 9 (92)% Loss: 1.626412272453308\n",
      "\n",
      "Test set: Average Loss:0.0071, Accuracy: 40.17%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 10 (15)% Loss: 1.5487613677978516\n",
      "Train Epoch: 10 (31)% Loss: 1.580745816230774\n",
      "Train Epoch: 10 (46)% Loss: 1.6376134157180786\n",
      "Train Epoch: 10 (61)% Loss: 1.551581621170044\n",
      "Train Epoch: 10 (77)% Loss: 1.5939388275146484\n",
      "Train Epoch: 10 (92)% Loss: 1.6090209484100342\n",
      "\n",
      "Test set: Average Loss:0.0073, Accuracy: 41.03%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 11 (15)% Loss: 1.5560376644134521\n",
      "Train Epoch: 11 (31)% Loss: 1.5048797130584717\n",
      "Train Epoch: 11 (46)% Loss: 1.48099946975708\n",
      "Train Epoch: 11 (61)% Loss: 1.476283073425293\n",
      "Train Epoch: 11 (77)% Loss: 1.6417783498764038\n",
      "Train Epoch: 11 (92)% Loss: 1.575188398361206\n",
      "\n",
      "Test set: Average Loss:0.0061, Accuracy: 44.13%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 12 (15)% Loss: 1.5828354358673096\n",
      "Train Epoch: 12 (31)% Loss: 1.4882392883300781\n",
      "Train Epoch: 12 (46)% Loss: 1.5608659982681274\n",
      "Train Epoch: 12 (61)% Loss: 1.607522964477539\n",
      "Train Epoch: 12 (77)% Loss: 1.6232430934906006\n",
      "Train Epoch: 12 (92)% Loss: 1.6366199254989624\n",
      "\n",
      "Test set: Average Loss:0.0060, Accuracy: 45.59%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 13 (15)% Loss: 1.4309086799621582\n",
      "Train Epoch: 13 (31)% Loss: 1.4941750764846802\n",
      "Train Epoch: 13 (46)% Loss: 1.391808032989502\n",
      "Train Epoch: 13 (61)% Loss: 1.4883612394332886\n",
      "Train Epoch: 13 (77)% Loss: 1.5520472526550293\n",
      "Train Epoch: 13 (92)% Loss: 1.4635478258132935\n",
      "\n",
      "Test set: Average Loss:0.0060, Accuracy: 46.3%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 14 (15)% Loss: 1.606181025505066\n",
      "Train Epoch: 14 (31)% Loss: 1.4218153953552246\n",
      "Train Epoch: 14 (46)% Loss: 1.4391756057739258\n",
      "Train Epoch: 14 (61)% Loss: 1.3893870115280151\n",
      "Train Epoch: 14 (77)% Loss: 1.4105466604232788\n",
      "Train Epoch: 14 (92)% Loss: 1.4400343894958496\n",
      "\n",
      "Test set: Average Loss:0.0061, Accuracy: 46.61%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 15 (15)% Loss: 1.371496558189392\n",
      "Train Epoch: 15 (31)% Loss: 1.345855474472046\n",
      "Train Epoch: 15 (46)% Loss: 1.383377194404602\n",
      "Train Epoch: 15 (61)% Loss: 1.4422168731689453\n",
      "Train Epoch: 15 (77)% Loss: 1.3178547620773315\n",
      "Train Epoch: 15 (92)% Loss: 1.5145081281661987\n",
      "\n",
      "Test set: Average Loss:0.0058, Accuracy: 48.57%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 16 (15)% Loss: 1.4844186305999756\n",
      "Train Epoch: 16 (31)% Loss: 1.3812768459320068\n",
      "Train Epoch: 16 (46)% Loss: 1.3403499126434326\n",
      "Train Epoch: 16 (61)% Loss: 1.3719085454940796\n",
      "Train Epoch: 16 (77)% Loss: 1.3978419303894043\n",
      "Train Epoch: 16 (92)% Loss: 1.4040113687515259\n",
      "\n",
      "Test set: Average Loss:0.0057, Accuracy: 49.49%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 17 (15)% Loss: 1.4097716808319092\n",
      "Train Epoch: 17 (31)% Loss: 1.3286783695220947\n",
      "Train Epoch: 17 (46)% Loss: 1.3282017707824707\n",
      "Train Epoch: 17 (61)% Loss: 1.3366719484329224\n",
      "Train Epoch: 17 (77)% Loss: 1.3081623315811157\n",
      "Train Epoch: 17 (92)% Loss: 1.3279134035110474\n",
      "\n",
      "Test set: Average Loss:0.0056, Accuracy: 50.19%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 18 (15)% Loss: 1.380053997039795\n",
      "Train Epoch: 18 (31)% Loss: 1.2853922843933105\n",
      "Train Epoch: 18 (46)% Loss: 1.2969485521316528\n",
      "Train Epoch: 18 (61)% Loss: 1.4583172798156738\n",
      "Train Epoch: 18 (77)% Loss: 1.349910020828247\n",
      "Train Epoch: 18 (92)% Loss: 1.3591094017028809\n",
      "\n",
      "Test set: Average Loss:0.0056, Accuracy: 50.69%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 19 (15)% Loss: 1.31419837474823\n",
      "Train Epoch: 19 (31)% Loss: 1.2848303318023682\n",
      "Train Epoch: 19 (46)% Loss: 1.2913874387741089\n",
      "Train Epoch: 19 (61)% Loss: 1.3515089750289917\n",
      "Train Epoch: 19 (77)% Loss: 1.2836534976959229\n",
      "Train Epoch: 19 (92)% Loss: 1.234159231185913\n",
      "\n",
      "Test set: Average Loss:0.0055, Accuracy: 50.68%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 20 (15)% Loss: 1.303574800491333\n",
      "Train Epoch: 20 (31)% Loss: 1.2662688493728638\n",
      "Train Epoch: 20 (46)% Loss: 1.2884286642074585\n",
      "Train Epoch: 20 (61)% Loss: 1.323995590209961\n",
      "Train Epoch: 20 (77)% Loss: 1.2149701118469238\n",
      "Train Epoch: 20 (92)% Loss: 1.187994122505188\n",
      "\n",
      "Test set: Average Loss:0.0057, Accuracy: 51.93%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 21 (15)% Loss: 1.1807615756988525\n",
      "Train Epoch: 21 (31)% Loss: 1.1778367757797241\n",
      "Train Epoch: 21 (46)% Loss: 1.2756119966506958\n",
      "Train Epoch: 21 (61)% Loss: 1.330867052078247\n",
      "Train Epoch: 21 (77)% Loss: 1.215561866760254\n",
      "Train Epoch: 21 (92)% Loss: 1.2304270267486572\n",
      "\n",
      "Test set: Average Loss:0.0063, Accuracy: 53.62%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 22 (15)% Loss: 1.2273038625717163\n",
      "Train Epoch: 22 (31)% Loss: 1.2218573093414307\n",
      "Train Epoch: 22 (46)% Loss: 1.1386412382125854\n",
      "Train Epoch: 22 (61)% Loss: 1.2906955480575562\n",
      "Train Epoch: 22 (77)% Loss: 1.1260261535644531\n",
      "Train Epoch: 22 (92)% Loss: 1.2862541675567627\n",
      "\n",
      "Test set: Average Loss:0.0056, Accuracy: 55.99%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 23 (15)% Loss: 1.29275643825531\n",
      "Train Epoch: 23 (31)% Loss: 1.2718075513839722\n",
      "Train Epoch: 23 (46)% Loss: 1.1482616662979126\n",
      "Train Epoch: 23 (61)% Loss: 1.202294945716858\n",
      "Train Epoch: 23 (77)% Loss: 1.2225373983383179\n",
      "Train Epoch: 23 (92)% Loss: 1.1579689979553223\n",
      "\n",
      "Test set: Average Loss:0.0051, Accuracy: 54.8%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 24 (15)% Loss: 1.1749368906021118\n",
      "Train Epoch: 24 (31)% Loss: 1.3464771509170532\n",
      "Train Epoch: 24 (46)% Loss: 1.1299073696136475\n",
      "Train Epoch: 24 (61)% Loss: 1.1688326597213745\n",
      "Train Epoch: 24 (77)% Loss: 1.186859369277954\n",
      "Train Epoch: 24 (92)% Loss: 1.1336441040039062\n",
      "\n",
      "Test set: Average Loss:0.0051, Accuracy: 55.2%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 25 (15)% Loss: 1.1083883047103882\n",
      "Train Epoch: 25 (31)% Loss: 1.1583296060562134\n",
      "Train Epoch: 25 (46)% Loss: 1.1302193403244019\n",
      "Train Epoch: 25 (61)% Loss: 1.2742533683776855\n",
      "Train Epoch: 25 (77)% Loss: 1.1266918182373047\n",
      "Train Epoch: 25 (92)% Loss: 1.0703147649765015\n",
      "\n",
      "Test set: Average Loss:0.0050, Accuracy: 56.3%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 26 (15)% Loss: 1.089413046836853\n",
      "Train Epoch: 26 (31)% Loss: 1.038691520690918\n",
      "Train Epoch: 26 (46)% Loss: 1.1904736757278442\n",
      "Train Epoch: 26 (61)% Loss: 1.150047779083252\n",
      "Train Epoch: 26 (77)% Loss: 1.0098562240600586\n",
      "Train Epoch: 26 (92)% Loss: 1.1271551847457886\n",
      "\n",
      "Test set: Average Loss:0.0051, Accuracy: 54.66%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 27 (15)% Loss: 1.0437495708465576\n",
      "Train Epoch: 27 (31)% Loss: 1.1029185056686401\n",
      "Train Epoch: 27 (46)% Loss: 1.0998094081878662\n",
      "Train Epoch: 27 (61)% Loss: 1.2703996896743774\n",
      "Train Epoch: 27 (77)% Loss: 1.0757638216018677\n",
      "Train Epoch: 27 (92)% Loss: 0.9738810062408447\n",
      "\n",
      "Test set: Average Loss:0.0047, Accuracy: 59.4%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 28 (15)% Loss: 1.1453397274017334\n",
      "Train Epoch: 28 (31)% Loss: 1.209020733833313\n",
      "Train Epoch: 28 (46)% Loss: 1.0761895179748535\n",
      "Train Epoch: 28 (61)% Loss: 0.9118561148643494\n",
      "Train Epoch: 28 (77)% Loss: 1.0563850402832031\n",
      "Train Epoch: 28 (92)% Loss: 1.107779622077942\n",
      "\n",
      "Test set: Average Loss:0.0049, Accuracy: 59.07%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 29 (15)% Loss: 1.1107048988342285\n",
      "Train Epoch: 29 (31)% Loss: 1.080622911453247\n",
      "Train Epoch: 29 (46)% Loss: 1.0329355001449585\n",
      "Train Epoch: 29 (61)% Loss: 0.9879353642463684\n",
      "Train Epoch: 29 (77)% Loss: 1.0633716583251953\n",
      "Train Epoch: 29 (92)% Loss: 1.098472237586975\n",
      "\n",
      "Test set: Average Loss:0.0052, Accuracy: 54.08%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 30 (15)% Loss: 0.990917444229126\n",
      "Train Epoch: 30 (31)% Loss: 1.0007959604263306\n",
      "Train Epoch: 30 (46)% Loss: 1.0237550735473633\n",
      "Train Epoch: 30 (61)% Loss: 1.069898009300232\n",
      "Train Epoch: 30 (77)% Loss: 1.1460801362991333\n",
      "Train Epoch: 30 (92)% Loss: 1.2115849256515503\n",
      "\n",
      "Test set: Average Loss:0.0065, Accuracy: 58.81%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 31 (15)% Loss: 0.9586231708526611\n",
      "Train Epoch: 31 (31)% Loss: 0.8718164563179016\n",
      "Train Epoch: 31 (46)% Loss: 0.9137023687362671\n",
      "Train Epoch: 31 (61)% Loss: 0.8416088819503784\n",
      "Train Epoch: 31 (77)% Loss: 1.0020897388458252\n",
      "Train Epoch: 31 (92)% Loss: 0.9012327790260315\n",
      "\n",
      "Test set: Average Loss:0.0054, Accuracy: 64.33%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 32 (15)% Loss: 0.892694354057312\n",
      "Train Epoch: 32 (31)% Loss: 0.952008068561554\n",
      "Train Epoch: 32 (46)% Loss: 0.9369450211524963\n",
      "Train Epoch: 32 (61)% Loss: 0.9487283825874329\n",
      "Train Epoch: 32 (77)% Loss: 0.9368307590484619\n",
      "Train Epoch: 32 (92)% Loss: 0.9231532216072083\n",
      "\n",
      "Test set: Average Loss:0.0041, Accuracy: 64.57%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 33 (15)% Loss: 0.9638152718544006\n",
      "Train Epoch: 33 (31)% Loss: 0.823805034160614\n",
      "Train Epoch: 33 (46)% Loss: 0.8526437282562256\n",
      "Train Epoch: 33 (61)% Loss: 0.9380545020103455\n",
      "Train Epoch: 33 (77)% Loss: 0.9550217986106873\n",
      "Train Epoch: 33 (92)% Loss: 0.8767784833908081\n",
      "\n",
      "Test set: Average Loss:0.0043, Accuracy: 64.76%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 34 (15)% Loss: 0.9033632278442383\n",
      "Train Epoch: 34 (31)% Loss: 1.019984483718872\n",
      "Train Epoch: 34 (46)% Loss: 0.9027735590934753\n",
      "Train Epoch: 34 (61)% Loss: 0.7783482670783997\n",
      "Train Epoch: 34 (77)% Loss: 0.9419573545455933\n",
      "Train Epoch: 34 (92)% Loss: 0.9999659657478333\n",
      "\n",
      "Test set: Average Loss:0.0078, Accuracy: 62.24%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 35 (15)% Loss: 0.9083480834960938\n",
      "Train Epoch: 35 (31)% Loss: 0.9436971545219421\n",
      "Train Epoch: 35 (46)% Loss: 0.7871504426002502\n",
      "Train Epoch: 35 (61)% Loss: 0.8997382521629333\n",
      "Train Epoch: 35 (77)% Loss: 0.8299765586853027\n",
      "Train Epoch: 35 (92)% Loss: 0.9764917492866516\n",
      "\n",
      "Test set: Average Loss:0.0042, Accuracy: 63.98%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 36 (15)% Loss: 0.8815680146217346\n",
      "Train Epoch: 36 (31)% Loss: 0.9494884610176086\n",
      "Train Epoch: 36 (46)% Loss: 0.780735433101654\n",
      "Train Epoch: 36 (61)% Loss: 0.9495514631271362\n",
      "Train Epoch: 36 (77)% Loss: 0.8068700432777405\n",
      "Train Epoch: 36 (92)% Loss: 1.030930757522583\n",
      "\n",
      "Test set: Average Loss:0.0039, Accuracy: 65.51%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 37 (15)% Loss: 0.82356858253479\n",
      "Train Epoch: 37 (31)% Loss: 0.8057013750076294\n",
      "Train Epoch: 37 (46)% Loss: 0.7955607771873474\n",
      "Train Epoch: 37 (61)% Loss: 0.9144575595855713\n",
      "Train Epoch: 37 (77)% Loss: 0.9892938137054443\n",
      "Train Epoch: 37 (92)% Loss: 0.808480441570282\n",
      "\n",
      "Test set: Average Loss:0.0039, Accuracy: 66.05%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 38 (15)% Loss: 0.9264061450958252\n",
      "Train Epoch: 38 (31)% Loss: 0.8735107779502869\n",
      "Train Epoch: 38 (46)% Loss: 0.9087310433387756\n",
      "Train Epoch: 38 (61)% Loss: 0.9709908962249756\n",
      "Train Epoch: 38 (77)% Loss: 0.9615690112113953\n",
      "Train Epoch: 38 (92)% Loss: 0.889419436454773\n",
      "\n",
      "Test set: Average Loss:0.0040, Accuracy: 65.36%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 39 (15)% Loss: 0.792993426322937\n",
      "Train Epoch: 39 (31)% Loss: 0.9965337514877319\n",
      "Train Epoch: 39 (46)% Loss: 0.856417715549469\n",
      "Train Epoch: 39 (61)% Loss: 0.892248272895813\n",
      "Train Epoch: 39 (77)% Loss: 0.8984726071357727\n",
      "Train Epoch: 39 (92)% Loss: 0.8615805506706238\n",
      "\n",
      "Test set: Average Loss:0.0038, Accuracy: 67.71%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 40 (15)% Loss: 0.8747313022613525\n",
      "Train Epoch: 40 (31)% Loss: 0.9445013403892517\n",
      "Train Epoch: 40 (46)% Loss: 0.8690751791000366\n",
      "Train Epoch: 40 (61)% Loss: 0.9184532761573792\n",
      "Train Epoch: 40 (77)% Loss: 0.814109742641449\n",
      "Train Epoch: 40 (92)% Loss: 0.8491461277008057\n",
      "\n",
      "Test set: Average Loss:0.0040, Accuracy: 66.85%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 41 (15)% Loss: 0.8742290139198303\n",
      "Train Epoch: 41 (31)% Loss: 0.8503561615943909\n",
      "Train Epoch: 41 (46)% Loss: 0.8574190735816956\n",
      "Train Epoch: 41 (61)% Loss: 0.7993842363357544\n",
      "Train Epoch: 41 (77)% Loss: 0.9069200754165649\n",
      "Train Epoch: 41 (92)% Loss: 0.7679595351219177\n",
      "\n",
      "Test set: Average Loss:0.0042, Accuracy: 67.03%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 42 (15)% Loss: 0.8582972288131714\n",
      "Train Epoch: 42 (31)% Loss: 0.8937941789627075\n",
      "Train Epoch: 42 (46)% Loss: 0.8819772005081177\n",
      "Train Epoch: 42 (61)% Loss: 0.8173387050628662\n",
      "Train Epoch: 42 (77)% Loss: 0.8520947694778442\n",
      "Train Epoch: 42 (92)% Loss: 0.9000873565673828\n",
      "\n",
      "Test set: Average Loss:0.0040, Accuracy: 67.39%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 43 (15)% Loss: 0.902221143245697\n",
      "Train Epoch: 43 (31)% Loss: 0.804376482963562\n",
      "Train Epoch: 43 (46)% Loss: 0.7645995020866394\n",
      "Train Epoch: 43 (61)% Loss: 0.8658455610275269\n",
      "Train Epoch: 43 (77)% Loss: 0.825682520866394\n",
      "Train Epoch: 43 (92)% Loss: 0.8475337624549866\n",
      "\n",
      "Test set: Average Loss:0.0037, Accuracy: 67.79%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 44 (15)% Loss: 0.801538348197937\n",
      "Train Epoch: 44 (31)% Loss: 0.8091751337051392\n",
      "Train Epoch: 44 (46)% Loss: 0.7526766657829285\n",
      "Train Epoch: 44 (61)% Loss: 0.8276358246803284\n",
      "Train Epoch: 44 (77)% Loss: 0.924446165561676\n",
      "Train Epoch: 44 (92)% Loss: 0.7934110164642334\n",
      "\n",
      "Test set: Average Loss:0.0041, Accuracy: 67.7%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 45 (15)% Loss: 0.8203295469284058\n",
      "Train Epoch: 45 (31)% Loss: 0.7987821102142334\n",
      "Train Epoch: 45 (46)% Loss: 0.787185788154602\n",
      "Train Epoch: 45 (61)% Loss: 0.9022877216339111\n",
      "Train Epoch: 45 (77)% Loss: 0.7483310699462891\n",
      "Train Epoch: 45 (92)% Loss: 0.7389774322509766\n",
      "\n",
      "Test set: Average Loss:0.0038, Accuracy: 67.17%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 46 (15)% Loss: 0.8358930349349976\n",
      "Train Epoch: 46 (31)% Loss: 0.8089249134063721\n",
      "Train Epoch: 46 (46)% Loss: 0.778828501701355\n",
      "Train Epoch: 46 (61)% Loss: 0.657737672328949\n",
      "Train Epoch: 46 (77)% Loss: 0.9331766963005066\n",
      "Train Epoch: 46 (92)% Loss: 0.8325816988945007\n",
      "\n",
      "Test set: Average Loss:0.0044, Accuracy: 66.69%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 47 (15)% Loss: 0.7283270955085754\n",
      "Train Epoch: 47 (31)% Loss: 0.870305597782135\n",
      "Train Epoch: 47 (46)% Loss: 0.7988649606704712\n",
      "Train Epoch: 47 (61)% Loss: 0.716884970664978\n",
      "Train Epoch: 47 (77)% Loss: 0.7803896069526672\n",
      "Train Epoch: 47 (92)% Loss: 0.6672893166542053\n",
      "\n",
      "Test set: Average Loss:0.0039, Accuracy: 67.88%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 48 (15)% Loss: 0.7664104104042053\n",
      "Train Epoch: 48 (31)% Loss: 0.7292658686637878\n",
      "Train Epoch: 48 (46)% Loss: 0.8562344908714294\n",
      "Train Epoch: 48 (61)% Loss: 0.6955559253692627\n",
      "Train Epoch: 48 (77)% Loss: 0.83240807056427\n",
      "Train Epoch: 48 (92)% Loss: 0.8228025436401367\n",
      "\n",
      "Test set: Average Loss:0.0040, Accuracy: 67.47%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 49 (15)% Loss: 0.7177026271820068\n",
      "Train Epoch: 49 (31)% Loss: 0.7566584944725037\n",
      "Train Epoch: 49 (46)% Loss: 0.7957503199577332\n",
      "Train Epoch: 49 (61)% Loss: 0.7703070640563965\n",
      "Train Epoch: 49 (77)% Loss: 0.751265287399292\n",
      "Train Epoch: 49 (92)% Loss: 0.7932662963867188\n",
      "\n",
      "Test set: Average Loss:0.0038, Accuracy: 68.7%\n",
      "\n",
      "==================================================\n",
      "Train Epoch: 50 (15)% Loss: 0.6948400735855103\n",
      "Train Epoch: 50 (31)% Loss: 0.715431809425354\n",
      "Train Epoch: 50 (46)% Loss: 0.7771497368812561\n",
      "Train Epoch: 50 (61)% Loss: 0.8051939010620117\n",
      "Train Epoch: 50 (77)% Loss: 0.8078849911689758\n",
      "Train Epoch: 50 (92)% Loss: 0.7882228493690491\n",
      "\n",
      "Test set: Average Loss:0.0037, Accuracy: 68.3%\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1,num_epochs+1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "033ea924f4de4a85902272c3b74cefbe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "03804903fb2345c6a2d0bd1926bbb0ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "090a3eb5c89d428abb284556df2b4e6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d7b9cb9984924749b2d05a45dabf3529",
       "IPY_MODEL_4a47abf147cd488c872092dd541e6cf3",
       "IPY_MODEL_16e0866dd52c415a88edbe4b7e324253"
      ],
      "layout": "IPY_MODEL_033ea924f4de4a85902272c3b74cefbe"
     }
    },
    "16e0866dd52c415a88edbe4b7e324253": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dbefad12d706444d8c69eb794ef1c800",
      "placeholder": "​",
      "style": "IPY_MODEL_26d1bc0dedcd43e1bebed25ab67fdef4",
      "value": " 170498071/170498071 [00:03&lt;00:00, 46265420.56it/s]"
     }
    },
    "26d1bc0dedcd43e1bebed25ab67fdef4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "356adf57b38e44f2879e3fc13d83949d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "474070fdd538445ca578d21d25ea89c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "4a47abf147cd488c872092dd541e6cf3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_356adf57b38e44f2879e3fc13d83949d",
      "max": 170498071,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_474070fdd538445ca578d21d25ea89c8",
      "value": 170498071
     }
    },
    "666fdad5a54044e291ced11586b40e95": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7b9cb9984924749b2d05a45dabf3529": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_03804903fb2345c6a2d0bd1926bbb0ca",
      "placeholder": "​",
      "style": "IPY_MODEL_666fdad5a54044e291ced11586b40e95",
      "value": "100%"
     }
    },
    "dbefad12d706444d8c69eb794ef1c800": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
